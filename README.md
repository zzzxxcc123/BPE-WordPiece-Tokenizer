# 토큰화 알고리즘 BPE, WordPiece Tokenizer
<br>

### 소개

토큰화는 자연어 처리(Natural Language Processing, NLP)에서 텍스트를 작은 단위로 나누는 과정으로, 자연어 처리 인공지능 학습에 용이하게 데이터를 수정/보완하는 중요한 단계입니다. 이 README는 토큰화 알고리즘 중 BPE, WordPiece Tokenizer에 대한 소개를 제공합니다.

<br>

### 토큰화의 필요성
자연어는 의미를 가진 단어, 문장 등의 의미 있는 단위로 구성되어 있습니다.
하지만 컴퓨터는 텍스트를 처리하기 어려워서, 토큰화는 텍스트를 이해 가능한 작은 부분으로 나누는 방법으로 이 문제를 해결합니다.
여러 이유로 인해 토큰화는 자연어 처리 작업의 성능을 향상시키는 데 중요한 역할을 합니다.

<br>
### 주요 이유

- 단어 단위의 처리: 

텍스트를 단어로 나누면 의미 있는 정보를 더 쉽게 추출할 수 있습니다.

- 텍스트 정규화:

토큰화를 통해 텍스트를 표준화하고 정규화할 수 있습니다.

- 텍스트 통계 수집: 

토큰화를 통해 어휘 크기, 단어 빈도 등의 텍스트 통계를 수집할 수 있습니다.

<br>

# BPE(Byte Pair Encoding)

- BPE(Byte pair encoding) 알고리즘은 1994년에 제안된 데이터 압축 알고리즘입니다. 하지만 후에 자연어 처리의 서브 워드 분리 알고리즘으로 응용되었습니다

- 자연어 처리에서의 BPE는 서브 워드 분리(subword segmentation), 즉 기존에 있던 단어를 분리하는 알고리즘입니다.

- Bottom-up 방식으로 작동하며, 글자(charcter) 단위에서 시작하여 단어 집합(vocabulary)을 만들어냅니다.

- 이 방법은 Sennrich et al가 2016년도에 서술한 'Neural Machine Translation of Rare Words with Subword Units'논문에서 제안되었습니다
- 논문: ![DenseNet121](https://arxiv.org/pdf/1508.07909.pdf) 

### BPE의 필요성
1. 어휘 크기 감소:

BPE는 어휘 크기를 동적으로 조절할 수 있습니다. 기존의 고정된 어휘 크기보다 더 작은 어휘 크기로 효과적으로 모델을 구성할 수 있습니다. 이는 효율적인 메모리 사용과 학습 속도를 제공하며, 작은 데이터 셋에서 특히 유용합니다.

2. 희귀한 단어 및 OOV 대처:

BPE는 희귀한 단어에 대한 대응력을 향상시킵니다. 새로운 단어나 희귀한 단어가 나타나면, BPE는 해당 단어를 부분 단어로 나누어 처리할 수 있습니다. 이는 미등록 단어(out-of-vocabulary)에 대한 강력한 대응을 제공합니다.

3. 언어 구조 이해:

BPE는 자연어의 구조를 더 잘 이해하게 도와줍니다. 단어의 부분 구조(subword)를 학습하므로, 모델은 단어 내의 의미적인 구성 요소를 학습할 수 있습니다.

4. 번역 품질 향상:

BPE를 사용하면 기계 번역 등의 작업에서 번역 품질이 향상될 수 있습니다. 희귀한 단어나 미등록 단어를 더 효과적으로 처리하며, 언어의 더 깊은 구조를 학습할 수 있습니다.

5. 유연성:

BPE는 언어에 종속되지 않으며, 다양한 언어에 적용할 수 있는 일반적인 접근 방식입니다. 이는 다국어 모델이나 언어 간 효율적인 지식 전이(transfer)에 유리합니다.


### BPE 실행 순서
1. 훈련 데이터로부터 단어 빈도수 카운트:
- BPE는 초기에 주어진 훈련 데이터에서 각 단어들의 빈도수를 계산합니다. 빈도수는 각 단어가 텍스트에서 얼마나 자주 등장하는지를 나타냅니다.
- {'apple': 8}

2. 모든 단어들을 글자(chracter) 단위로 분리:
- 훈련 데이터에 있는 모든 단어들을 글자 단위로 분리합니다. 이로써 초기 단어 집합(vocabulary)은 글자들의 시퀀스로 구성됩니다.
- 'apple' -> 'a', 'p', 'p', 'l', 'e'
- vocabulary: a, p, p, l, e

4. 빈도가 가장 높은 글자(chracter) 쌍을 하나의 글자(chracter)로 통합:
- 빈도가 가장 높은 글자(chracter) 쌍을 찾아서 하나의 새로운 글자(chracter)로 통합합니다. 이때, 이미 통합된 글자(chracter)들도 고려하여 빈도수를 업데이트합니다. 이 과정을 미리 정해진 횟수나 어떤 기준을 충족할 때까지 반복합니다.
- ('a', 'p', 'p', 'l', 'e') -> 'ap', 'pp', 'pl', 'le' 중 'ap'가 가장 많은 가정 하에 업데이트
- 'a', 'p', 'p', 'l', 'e' -> 'ap', 'p', 'l', 'e'

4. 반복:
- 단계 3의 과정을 원하는 횟수나 기준을 충족할 때까지 반복합니다. 이를 통해 텍스트 데이터의 패턴이나 서브워드를 효과적으로 학습하며, 자주 등장하는 패턴을 하나의 새로운 토큰으로 통합하여 어휘 크기를 줄이는 효과를 얻습니다.




